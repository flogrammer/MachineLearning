{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import some necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from random import randrange\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train an agent with Reinforcement Learning we need an environment the agent can act in and that gives the agent feedback for its actions.\n",
    "\n",
    "For this we are going to use the PyGame Learning Environment (PLE): https://github.com/ntasfi/PyGame-Learning-Environment.\n",
    "\n",
    "To install this, first pygame has to be installed: https://www.pygame.org/wiki/GettingStarted#Pygame%20Installation\n",
    "\n",
    "Follow the install instructions for pygame and install the PLE afterwards. If you have done everything correctly, the following imports should work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ple import PLE # the main wrapper for the environment\n",
    "from ple.games.flappybird import FlappyBird # a single game environment, you can try others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can initialize a game and environment instance. After executing the following code, a black window should appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = FlappyBird()\n",
    "env = PLE(game, fps=30, display_screen=True, force_fps=False)\n",
    "env.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game waits for our agent to pick and execute an action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "while not env.game_over():\n",
    "    reward = env.act(119) # do a \"Flap\"\n",
    "    #reward = env.act(None) # do nothing\n",
    "env.reset_game()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to read out the current game state of the environment, we need to pass the environment a preprocessor-function, that translates the dictionary given by the environment into a numpy matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessor(game_state):\n",
    "    # the game state for FlappyBird is a dictionary with 8 entries:\n",
    "    #    - player_y                      : the y-position of the bird\n",
    "    #    - player_vel                    : the velocity of the bird (pointed downwards)\n",
    "    #    - next_pipe_dist_to_player      : distance to the next pipe\n",
    "    #    - next_pipe_top_y               : top position of the next pipe\n",
    "    #    - next_pipe_bottom_y            : bottom position of the next pipe\n",
    "    #    - next_next_pipe_dist_to_player : distance to the next next pipe\n",
    "    #    - next_next_pipe_top_y          : top position of the next next pipe\n",
    "    #    - next_next_pipe_bottom_y       : bottom position of the next next pipe\n",
    "    \n",
    "    # For our purposes it is better, if we normalize the input\n",
    "    # Screen-Height: 512\n",
    "    # Screen-Width: 288\n",
    "    # Distance to next next pipe: 1.65 * Screen_Width\n",
    "    total_height = 512\n",
    "    total_width = 288\n",
    "    max_velocity = 10   \n",
    "    \n",
    "    preprocessed_state = np.empty((1, 8), dtype=float)\n",
    "    preprocessed_state[0][0] = float(game_state[\"player_y\"]/total_height)\n",
    "    preprocessed_state[0][1] = float(game_state[\"player_vel\"]/max_velocity)\n",
    "    preprocessed_state[0][2] = float(game_state[\"next_pipe_dist_to_player\"]/total_width)\n",
    "    preprocessed_state[0][3] = float(game_state[\"next_pipe_top_y\"]/total_height)\n",
    "    preprocessed_state[0][4] = float(game_state[\"next_pipe_bottom_y\"]/total_height)\n",
    "    preprocessed_state[0][5] = float(game_state[\"next_next_pipe_dist_to_player\"]/(total_width*1.65))\n",
    "    preprocessed_state[0][6] = float(game_state[\"next_next_pipe_top_y\"]/total_height)\n",
    "    preprocessed_state[0][7] = float(game_state[\"next_next_pipe_bottom_y\"]/total_height)\n",
    "\n",
    "    return preprocessed_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can initialize the environment with this preprocessor and read out the game state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = PLE(game, fps=30, display_screen=True, force_fps=False,\n",
    "         state_preprocessor=preprocessor)\n",
    "env.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5       ,  0.1       ,  0.98263889,  0.375     ,  0.5703125 ,\n",
       "         0.89856902,  0.14453125,  0.33984375]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.getGameState()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to define an agent, that is able to interact and learn with the environment.\n",
    "\n",
    "We can either use a Q-Table approach, storing the Q-values in a \"state-lookup-table\" or use an estimator, like a neural network. We are going to use the latter. \n",
    "\n",
    "EXERCISE: Give an explanation why a table approach might be insufficient. What does an estimator do better?\n",
    "\n",
    "--> https://www.quora.com/In-what-way-can-Q-learning-and-neural-networks-work-together\n",
    "--> Q Table can be insufficiently large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RLAgent():\n",
    "    \n",
    "    # the constructor for our agent\n",
    "    def __init__(self, actions):\n",
    "        self.actions = actions # [119, None] for FlappyBird\n",
    "        self.q_estimator = self.getQEstimator()\n",
    "        self.discount_factor = 0.95\n",
    "        self.number_of_updates = 0\n",
    "        \n",
    "    def getQEstimator(self):\n",
    "        # MLPRegressor from sklearn.neural_network \n",
    "        # In the constructor various variables can be set, most importantly the number and size of the hidden layers.\n",
    "        # E.g.: hidden_layer_sizes=(100,100) would create two hidden layers with 100 neurons each.\n",
    "        # Hint: To correctly initialize the estimator, call its fit-function after construction\n",
    "        # with \"dummy\"-matrices in the shape of the designated input and output.\n",
    "        \n",
    "        input_dummy = np.array(0.5,  0.1,  0.98263889,  0.375,  0.5703125, 0.89856902, 0.14453125,  0.33984375)\n",
    "        \n",
    "        estimator = MLPRegressor(hidden_layer_sizes=(100,))\n",
    "        estimator.fit(input_dummy, 1)\n",
    "        \n",
    "        return estimator\n",
    "    \n",
    "    def pickAction(self, state, select_greedily=False):\n",
    "        # EXERCISE: During training we want a trade-off between exploration and exploitation.\n",
    "        # To achieve this, implement a function that by chance selects an action randomly.\n",
    "        # That chance should be very high at the beginning of training and decrease over the number of update steps.\n",
    "        # You can use self.number_of_updates to determine how likely a random choice should be.\n",
    "        # If the selection is not random, then the action with the maximum Q-value should be chosen (see np.argmax).\n",
    "        \n",
    "        # Predict the Q-values with our agent's estimator.\n",
    "        q_values = np.zeros((1, len(self.actions))) # <= dummy implementation\n",
    "        \n",
    "        random_action_index = randrange(0, len(self.actions))\n",
    "        \n",
    "        # Return the selected action and the predicted Q-values.\n",
    "        return self.actions[random_action_index], q_values\n",
    "        \n",
    "    def update(self, state, action, sucessor_state, reward, is_terminal, q_values):\n",
    "        \n",
    "        # Mapping the action to the corresponding index\n",
    "        action_index = self.actions.index(action)\n",
    "        \n",
    "        # Initialize target vector with old Q-values.\n",
    "        target = np.zeros((1, len(self.actions)))\n",
    "        target[:] = q_values[:]\n",
    "        \n",
    "        # EXERCISE: \n",
    "        # To update the Q-value estimation, we need to implement the Q-Learning algorithm (see the lecture slides).\n",
    "        # We already have the state \"s\", a selected action \"a\", the reward \"r\" and the new state \"s'\".\n",
    "        # Implement the update step according to the Q-Learning algorithm.\n",
    "        # A few hints:\n",
    "        # 1. If a terminal state has been reached, then the target value is only the received reward (Q(s,a) <- reward).\n",
    "        # 2. Only update the Q-value for the chosen action and don't forget to keep the other values unchanged.\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.number_of_updates += 1\n",
    "        \n",
    "       \n",
    "    def save(self, iteration):\n",
    "        filepath = \"save_\" + str(iteration) + \".pkl\"\n",
    "        with open(filepath, \"wb\") as pkl:\n",
    "            pickle.dump(self.q_estimator, pkl)\n",
    "            \n",
    "    def load(self, iteration):\n",
    "        filepath = \"save_\" + str(iteration) + \".pkl\"\n",
    "        with open(filepath, \"rb\") as pkl:\n",
    "            self.q_estimator = pickle.load(pkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we can train our agent by playing a number of episodes and updating the Q-value estimation step by step.\n",
    "\n",
    "The following code block executes a defined number of episodes and trains the agent using the methods defined above.\n",
    "You can and should add a few things:\n",
    "\n",
    "* Save the agent's Q-estimator during training in regular intervals (e.g. after every x-th update).\n",
    "* Print out interesting metrics, like (average) reward per episode or episode length.\n",
    "* Add a few evaluation steps during or after training that always picks actions greedily.\n",
    "* Tune the various parameters for the MLP (net layout, activation-function, ...) or the environment (reward-values, frame_skip, ...).\n",
    "* If you have reached a good performing agent, try another game environemtn.\n",
    "\n",
    "The training can take a while before the agent shows some kind of performance improvement, but after at most a few hours you should be able to make out some improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only 2 non-keyword arguments accepted",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-82-02dc2ebcf733>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRLAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetActionSet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[0mepisode_counter\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mtraining_episodes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-81-1c44e6c656a4>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactions\u001b[0m \u001b[1;31m# [119, None] for FlappyBird\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq_estimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetQEstimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscount_factor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.95\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumber_of_updates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-81-1c44e6c656a4>\u001b[0m in \u001b[0;36mgetQEstimator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m# with \"dummy\"-matrices in the shape of the designated input and output.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0minput_dummy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;36m0.98263889\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;36m0.375\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;36m0.5703125\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.89856902\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.14453125\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;36m0.33984375\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMLPRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_layer_sizes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: only 2 non-keyword arguments accepted"
     ]
    }
   ],
   "source": [
    "training_episodes = 100\n",
    "episode_counter = 0\n",
    "\n",
    "# This contains the rewards given to the agent based on different actions in game.\n",
    "# Tick is rewarded on each time step, while the others are awarded based on the game state.\n",
    "# E.g.: passing a pipe results in a positive reward while touching a pipe will result in a loss.\n",
    "# You can adjust the reward to achieve a better result. (There is no win or negative loss in the FlappyBird game.)\n",
    "reward_values = {\"positive\" : 1.0,\n",
    "                 \"negative\" : -1.0,\n",
    "                 \"tick\": 0.0,\n",
    "                 \"loss\" : -5.0,\n",
    "                 \"win\" : 5.0}\n",
    "\n",
    "# The number of frames that an action is executed on.\n",
    "# For a value of 1, a new action is chosen for every frame.\n",
    "# For a value of 4, a new action is chosen every 4th frame and then executed for 4 frames.\n",
    "# A higher number leads to a slower reaction of our agent, \n",
    "# but increases the influence one selection has and also allows for a faster computation.\n",
    "frame_skip = 1\n",
    "\n",
    "# If you want to train without a window opening:\n",
    "# 1. Set display_screen=False\n",
    "# 2. Uncomment the following line of code:\n",
    "# os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "\n",
    "# If you want to evaluate in \"normal\" speed while watching the agent on the screen:\n",
    "# 1. Set display_screen=True\n",
    "# 2. Set force_fps=False\n",
    "env = PLE(game, fps=30, display_screen=True, force_fps=True, \n",
    "         state_preprocessor=preprocessor,\n",
    "         frame_skip=1,\n",
    "         reward_values=reward_values)\n",
    "env.init()\n",
    "\n",
    "agent = RLAgent(env.getActionSet())\n",
    "\n",
    "while episode_counter < training_episodes:\n",
    "    \n",
    "    state = env.getGameState()\n",
    "    \n",
    "    # Pick an action based on the state and execute it in the environment to receive the reward.\n",
    "    action, q_values = agent.pickAction(state, select_greedily=False)\n",
    "    reward = env.act(action) \n",
    "    \n",
    "    successor_state = env.getGameState()\n",
    "    is_terminal = env.game_over()\n",
    "    \n",
    "    # Update the agent's Q-value estimation using the current observation.\n",
    "    agent.update(state, action, successor_state, reward, is_terminal, q_values)\n",
    "    \n",
    "    if is_terminal:\n",
    "        env.reset_game()\n",
    "        episode_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
