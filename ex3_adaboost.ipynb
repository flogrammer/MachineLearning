{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     preg  plas  pres  skin  test  mass   pedi  age  class\n",
      "0       6   148    72    35     0  33.6  0.627   50      1\n",
      "1       1    85    66    29     0  26.6  0.351   31      0\n",
      "2       8   183    64     0     0  23.3  0.672   32      1\n",
      "3       1    89    66    23    94  28.1  0.167   21      0\n",
      "4       0   137    40    35   168  43.1  2.288   33      1\n",
      "5       5   116    74     0     0  25.6  0.201   30      0\n",
      "6       3    78    50    32    88  31.0  0.248   26      1\n",
      "7      10   115     0     0     0  35.3  0.134   29      0\n",
      "8       2   197    70    45   543  30.5  0.158   53      1\n",
      "9       8   125    96     0     0   0.0  0.232   54      1\n",
      "10      4   110    92     0     0  37.6  0.191   30      0\n",
      "11     10   168    74     0     0  38.0  0.537   34      1\n",
      "12     10   139    80     0     0  27.1  1.441   57      0\n",
      "13      1   189    60    23   846  30.1  0.398   59      1\n",
      "14      5   166    72    19   175  25.8  0.587   51      1\n",
      "15      7   100     0     0     0  30.0  0.484   32      1\n",
      "16      0   118    84    47   230  45.8  0.551   31      1\n",
      "17      7   107    74     0     0  29.6  0.254   31      1\n",
      "18      1   103    30    38    83  43.3  0.183   33      0\n",
      "19      1   115    70    30    96  34.6  0.529   32      1\n",
      "20      3   126    88    41   235  39.3  0.704   27      0\n",
      "21      8    99    84     0     0  35.4  0.388   50      0\n",
      "22      7   196    90     0     0  39.8  0.451   41      1\n",
      "23      9   119    80    35     0  29.0  0.263   29      1\n",
      "24     11   143    94    33   146  36.6  0.254   51      1\n",
      "25     10   125    70    26   115  31.1  0.205   41      1\n",
      "26      7   147    76     0     0  39.4  0.257   43      1\n",
      "27      1    97    66    15   140  23.2  0.487   22      0\n",
      "28     13   145    82    19   110  22.2  0.245   57      0\n",
      "29      5   117    92     0     0  34.1  0.337   38      0\n",
      "..    ...   ...   ...   ...   ...   ...    ...  ...    ...\n",
      "738     2    99    60    17   160  36.6  0.453   21      0\n",
      "739     1   102    74     0     0  39.5  0.293   42      1\n",
      "740    11   120    80    37   150  42.3  0.785   48      1\n",
      "741     3   102    44    20    94  30.8  0.400   26      0\n",
      "742     1   109    58    18   116  28.5  0.219   22      0\n",
      "743     9   140    94     0     0  32.7  0.734   45      1\n",
      "744    13   153    88    37   140  40.6  1.174   39      0\n",
      "745    12   100    84    33   105  30.0  0.488   46      0\n",
      "746     1   147    94    41     0  49.3  0.358   27      1\n",
      "747     1    81    74    41    57  46.3  1.096   32      0\n",
      "748     3   187    70    22   200  36.4  0.408   36      1\n",
      "749     6   162    62     0     0  24.3  0.178   50      1\n",
      "750     4   136    70     0     0  31.2  1.182   22      1\n",
      "751     1   121    78    39    74  39.0  0.261   28      0\n",
      "752     3   108    62    24     0  26.0  0.223   25      0\n",
      "753     0   181    88    44   510  43.3  0.222   26      1\n",
      "754     8   154    78    32     0  32.4  0.443   45      1\n",
      "755     1   128    88    39   110  36.5  1.057   37      1\n",
      "756     7   137    90    41     0  32.0  0.391   39      0\n",
      "757     0   123    72     0     0  36.3  0.258   52      1\n",
      "758     1   106    76     0     0  37.5  0.197   26      0\n",
      "759     6   190    92     0     0  35.5  0.278   66      1\n",
      "760     2    88    58    26    16  28.4  0.766   22      0\n",
      "761     9   170    74    31     0  44.0  0.403   43      1\n",
      "762     9    89    62     0     0  22.5  0.142   33      0\n",
      "763    10   101    76    48   180  32.9  0.171   63      0\n",
      "764     2   122    70    27     0  36.8  0.340   27      0\n",
      "765     5   121    72    23   112  26.2  0.245   30      0\n",
      "766     1   126    60     0     0  30.1  0.349   47      1\n",
      "767     1    93    70    31     0  30.4  0.315   23      0\n",
      "\n",
      "[768 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pd.read_csv(url, names=names)\n",
    "print(dataframe)\n",
    "data_values = dataframe.values # 768 samples, 9 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   6.     148.      72.    ...,    0.627   50.       1.   ]\n",
      " [   1.      85.      66.    ...,    0.351   31.       0.   ]\n",
      " [   8.     183.      64.    ...,    0.672   32.       1.   ]\n",
      " ..., \n",
      " [   5.     121.      72.    ...,    0.245   30.       0.   ]\n",
      " [   1.     126.      60.    ...,    0.349   47.       1.   ]\n",
      " [   1.      93.      70.    ...,    0.315   23.       0.   ]]\n",
      "(768, 9)\n"
     ]
    }
   ],
   "source": [
    "print(data_values)\n",
    "print(data_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "feature_matrix = data_values[:,0:8] #Without classification\n",
    "labels = data_values[:,8] #Labels (classification)\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the first X elements as train and the otherones as test data\n",
    "train_features = feature_matrix[:450] #SYNTAX: array[start:stop:step]\n",
    "train_labels = labels[:450]\n",
    "\n",
    "test_features = feature_matrix[450:]\n",
    "test_labels = labels[450:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.922222222222\n",
      "0.691823899371\n"
     ]
    }
   ],
   "source": [
    "#Train \"weak\" classifier as decision tree\n",
    "clf = DecisionTreeClassifier(max_depth=9, min_samples_leaf=1)#from sckit learn\n",
    "clf.fit(train_features,train_labels)\n",
    "clf_error_train = clf.score(train_features, train_labels)\n",
    "clf_error_test = clf.score(test_features,test_labels)\n",
    "print(clf_error_train)\n",
    "print(clf_error_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_error_rate(pred, Y):\n",
    "    return sum(pred != Y) / float(len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_error_rate(er_train, er_test):\n",
    "    df_error = pd.DataFrame([er_train, er_test]).T\n",
    "    df_error.columns = ['Training', 'Test']\n",
    "    plot1 = df_error.plot(linewidth = 3, figsize = (8,6),\n",
    "            color = ['lightblue', 'darkblue'], grid = True)\n",
    "    plot1.set_xlabel('Number of iterations', fontsize = 12)\n",
    "    plot1.set_xticklabels(range(0,450,50))\n",
    "    plot1.set_ylabel('Error rate', fontsize = 12)\n",
    "    plot1.set_title('Error rate vs number of iterations', fontsize = 16)\n",
    "    plt.axhline(y=er_test[0], linewidth=1, color = 'red', ls = 'dashed')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXCERSICE 2\n",
    "#Implement the adaboost algorithm\n",
    "def adaboost_clf(train_features, train_labels, test_features, test_labels, M, clf):\n",
    "    #M is the number of iterations\n",
    "    #clf is the decision tree classifier\n",
    "    n_train, n_test = len(train_features), len(test_features)\n",
    "    # Set initial weights, for each datapoint = 1/n\n",
    "    w = np.ones(n_train) / n_train #np.ones returns array of ones\n",
    "    pred_train, pred_test = [np.zeros(n_train), np.zeros(n_test)]\n",
    "    \n",
    "    # Step 1 / 6\n",
    "    for i in range(M):\n",
    "        # Step 2 \n",
    "        # Fit a classifier with the specific weights\n",
    "        clf.fit(train_features, train_labels, sample_weight=w)\n",
    "    \n",
    "        pred_train_i = clf.predict(train_features) \n",
    "        pred_test_i = clf.predict(test_features)     \n",
    "        \n",
    "        #Calculate empiric error\n",
    "        # Step 3\n",
    "        # Indicator function which represents the missclassification of weak classifier\n",
    "        \n",
    "        miss = [float(pred_train_i[x] != train_labels[x]) for x in range(0,len(pred_train_i))]\n",
    "        \n",
    "        # Equivalent with 1/-1 to update weights\n",
    "        miss2 = np.array([x if x==1 else -1 for x in miss])\n",
    "        # Error \n",
    "        err_m = np.dot(w,miss) / sum(w)\n",
    "        # Step 4\n",
    "        # Alpha\n",
    "        alpha_m = 0.5*np.log((1-err_m)/float(err_m))\n",
    "        # Step 5\n",
    "        # New weights\n",
    "        w = w*np.exp(miss2*alpha_m)\n",
    "        w = w / np.sum(w)\n",
    "        # Ergebnis: \n",
    "        # Add to prediction\n",
    "        pred_train = pred_train + alpha_m*pred_train_i\n",
    "        pred_test = pred_test + alpha_m*pred_test_i\n",
    "    #write´the sign of the predictions\n",
    "    pred_train, pred_test = np.sign(pred_train), np.sign(pred_test)\n",
    "    # Return error rate in train and test set\n",
    "    return get_error_rate(pred_train, train_labels), \\\n",
    "           get_error_rate(pred_test, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "er_train, er_test = [clf_error_train], [clf_error_test]\n",
    "# Fit Adaboost classifier using a decision tree as base estimator\n",
    " # Test with different number of iterations\n",
    "x_range = range(10,450,10)\n",
    "for i in x_range:\n",
    "        er_i = adaboost_clf(train_features, train_labels, test_features, test_labels, i, clf)\n",
    "        er_train.append(er_i[0])\n",
    "        er_test.append(er_i[1])\n",
    "    \n",
    "# Compare error rate vs number of iterations\n",
    "plot_error_rate(er_train, er_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
